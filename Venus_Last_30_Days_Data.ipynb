{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import datetime\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "#Download Code from IE\n",
    "def download_file(url, filename):\n",
    "    ''' Downloads file from the url and save it as filename '''\n",
    "    # check if file already exists\n",
    "    if not os.path.isfile(filename):\n",
    "        #print('Downloading File')\n",
    "        response = requests.get(url)\n",
    "        # Check if the response is ok (200)\n",
    "        if response.status_code == 200:\n",
    "            # Open file and write the content\n",
    "            with open(filename, 'wb') as file:\n",
    "                # A chunk of 128 bytes\n",
    "                for chunk in response:\n",
    "                    file.write(chunk)\n",
    "    else:\n",
    "        print('File exists')       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n",
      "File exists\n",
      "15JUN2019 It was holiday, so date has not considered\n",
      "16JUN2019 It was holiday, so date has not considered\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "22JUN2019 It was holiday, so date has not considered\n",
      "23JUN2019 It was holiday, so date has not considered\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "29JUN2019 It was holiday, so date has not considered\n",
      "30JUN2019 It was holiday, so date has not considered\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "06JUL2019 It was holiday, so date has not considered\n",
      "07JUL2019 It was holiday, so date has not considered\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "13JUL2019 It was holiday, so date has not considered\n",
      "14JUL2019 It was holiday, so date has not considered\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "20JUL2019 It was holiday, so date has not considered\n",
      "21JUL2019 It was holiday, so date has not considered\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "27JUL2019 It was holiday, so date has not considered\n",
      "28JUL2019 It was holiday, so date has not considered\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "03AUG2019 It was holiday, so date has not considered\n",
      "04AUG2019 It was holiday, so date has not considered\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "10AUG2019 It was holiday, so date has not considered\n",
      "11AUG2019 It was holiday, so date has not considered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raviraj.Raviraj-PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[376.185,\n",
       " 391.77000000000004,\n",
       " 407.355,\n",
       " 422.94000000000005,\n",
       " 438.52500000000003,\n",
       " 454.11,\n",
       " 469.69500000000005,\n",
       " 485.28000000000003,\n",
       " 500.865]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, os\n",
    "## Venus Project - download Last 30 working days Days of data \n",
    "cwd = os.getcwd()\n",
    "#Last 30 Days Dates\n",
    "from datetime import timedelta, date\n",
    "vDateval=60\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "start_date = datetime.date.today() + datetime.timedelta(-vDateval)#date(2019, 1, 1)\n",
    "end_date = datetime.date.today()#date(2015, 6, 2)\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    Date_itr=single_date.strftime(\"%d\"+(single_date.strftime('%b').upper())+'%Y')\n",
    "    filename = cwd+'\\\\cm{}bhav.csv.zip'.format(Date_itr)\n",
    "    Yesterday=(single_date.strftime(\"%A\"))\n",
    "    #print(Yesterday + \" \" + Date_itr)\n",
    "    if Yesterday != 'Saturday' and Yesterday != 'Sunday':\n",
    "        url = 'https://www.nseindia.com/content/historical/EQUITIES/2019/'+single_date.strftime('%b').upper()+'/cm{}bhav.csv.zip'.format(Date_itr)\n",
    "        download_file(url,filename)\n",
    "        try:\n",
    "            with ZipFile(filename, 'r') as zf:\n",
    "                zf.extractall(cwd)\n",
    "                #df=pd.read_csv(filename)\n",
    "                #df.head()\n",
    "        except:\n",
    "            print(Date_itr + \" Data un-available\")\n",
    "    else:\n",
    "        print(Date_itr + \" It was holiday, so date has not considered\")\n",
    "\n",
    "df = pd.concat(map(pd.read_csv, glob.glob(os.path.join('', cwd+\"\\\\*.csv\"))))\n",
    "a=df\n",
    "#Perform other Operations\n",
    "alldata=df[df.SYMBOL=='TATASTEEL']\n",
    "alldata\n",
    "alldata=alldata[alldata.SERIES=='EQ']\n",
    "alldata['MinVal']=alldata[['OPEN','HIGH','LOW','CLOSE']].min(axis=1)\n",
    "min_val=alldata['MinVal']\n",
    "alldata['MaxVal']=alldata[['OPEN','HIGH','LOW','CLOSE','LAST']].max(axis=1)\n",
    "max_val=alldata['MaxVal']\n",
    "alldata.drop(labels=['SERIES','LAST','PREVCLOSE',\"TOTTRDQTY\",\"TOTTRDVAL\",\"TIMESTAMP\",\"TOTALTRADES\",\"ISIN\"],axis=1)\n",
    "\n",
    "#Calculate difference Percentage\n",
    "min_val=alldata.LOW.min()\n",
    "max_val=alldata.HIGH.max()\n",
    "diff_val=max_val-min_val\n",
    "diff_per=0\n",
    "\n",
    "Diff_values=np.linspace(min_val,max_val,11)\n",
    "Col_Rows=['Min Val','10%','20%','30%','40%','50%','60%','70%','80%','90%','Max']\n",
    "Diff_values=Diff_values.reshape(1,11)\n",
    "df=pd.DataFrame(data=Diff_values,columns=Col_Rows)\n",
    "\n",
    "low_Timestamp=alldata[alldata['LOW']==min_val]['TIMESTAMP'].values\n",
    "High_Timestamp=alldata[alldata['HIGH']==max_val]['TIMESTAMP'].values\n",
    "import datetime\n",
    "start_date=datetime.datetime.strptime(low_Timestamp[0], \"%d-%b-%Y\")\n",
    "end_date=datetime.datetime.strptime(High_Timestamp[0], \"%d-%b-%Y\")\n",
    "\n",
    "lst=[10,20,30,40,50,60,70,80,90,100]\n",
    "Per_Val=[]\n",
    "for i in range(0,9):\n",
    "    Per_Val.append(((max_val-min_val)*lst[i]/100)+min_val)\n",
    "Per_Val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
